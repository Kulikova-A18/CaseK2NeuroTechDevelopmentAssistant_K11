{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07a46458-eeb5-4024-a8dd-6747d2ba42fe",
   "metadata": {},
   "source": [
    "## EVALUATION RUNNING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ce21a8c-919d-4797-a62e-1c8fa5d98388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DAILY EVALUATION (Layer A) ===\n",
      "\n",
      "JSON validity rate: 93.75%\n",
      "Clarification precision: 62.50%\n",
      "Clarification recall:    83.33%\n",
      "False clarification rate:33.33%\n",
      "Task ID correctness: 85.71%\n",
      "\n",
      "Quality distribution:\n",
      "  DETAIL_OK: 56.25%\n",
      "  GREAT: 12.50%\n",
      "  TOO_SHORT: 18.75%\n",
      "  EMPTY: 6.25%\n",
      "\n",
      "Raw stats:\n",
      "{'action_success': 7, 'clar_tn': 6, 'json_valid': 15, 'task_id_valid': 6, 'action_ask_clarification': 8, 'clar_tp': 5, 'clar_fp': 3, 'task_id_invalid': 1, 'clar_fn': 1, 'runtime_error': 1}\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Imports\n",
    "# =========================\n",
    "from evaluation.eval_runner import run_daily_eval\n",
    "from evaluation.daily_cases import DAILY_CASES\n",
    "from llm_core.agent_process import agent_process\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# =========================\n",
    "# Run DAILY evaluation\n",
    "# =========================\n",
    "daily_stats, quality_dist = run_daily_eval(DAILY_CASES)\n",
    "\n",
    "total_cases = len(DAILY_CASES)\n",
    "\n",
    "print(\"=== DAILY EVALUATION (Layer A) ===\\n\")\n",
    "\n",
    "# JSON validity\n",
    "json_valid_rate = daily_stats[\"json_valid\"] / total_cases\n",
    "print(f\"JSON validity rate: {json_valid_rate:.2%}\")\n",
    "\n",
    "# Clarification metrics\n",
    "tp = daily_stats[\"clar_tp\"]\n",
    "fp = daily_stats[\"clar_fp\"]\n",
    "fn = daily_stats[\"clar_fn\"]\n",
    "tn = daily_stats[\"clar_tn\"]\n",
    "\n",
    "clar_precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "clar_recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "false_clar_rate = fp / (fp + tn) if (fp + tn) > 0 else 0.0\n",
    "\n",
    "print(f\"Clarification precision: {clar_precision:.2%}\")\n",
    "print(f\"Clarification recall:    {clar_recall:.2%}\")\n",
    "print(f\"False clarification rate:{false_clar_rate:.2%}\")\n",
    "\n",
    "# Task ID correctness (только SUCCESS внутри eval)\n",
    "task_valid = daily_stats[\"task_id_valid\"]\n",
    "task_invalid = daily_stats[\"task_id_invalid\"]\n",
    "task_id_correctness = (\n",
    "    task_valid / (task_valid + task_invalid)\n",
    "    if (task_valid + task_invalid) > 0\n",
    "    else 0.0\n",
    ")\n",
    "\n",
    "print(f\"Task ID correctness: {task_id_correctness:.2%}\")\n",
    "\n",
    "# Quality distribution (diagnostic)\n",
    "print(\"\\nQuality distribution:\")\n",
    "for quality, count in quality_dist.items():\n",
    "    share = count / total_cases\n",
    "    print(f\"  {quality}: {share:.2%}\")\n",
    "\n",
    "print(\"\\nRaw stats:\")\n",
    "print(dict(daily_stats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a50dc86d-33d3-4957-88f2-c26e0d612e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ANALYTICS EVALUATION (Layer A) ===\n",
      "\n",
      "Intent accuracy (supported): 92.31%\n",
      "Unsupported intent correctness: 100.00%\n",
      "\n",
      "Raw stats:\n",
      "{'intent_correct': 12, 'intent_wrong': 1, 'unsupported_correct': 5}\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# ANALYTICS EVALUATION\n",
    "# =========================\n",
    "from evaluation.eval_runner import run_analytics_eval\n",
    "from evaluation.analytics_cases import ANALYTICS_CASES\n",
    "\n",
    "analytics_stats = run_analytics_eval(ANALYTICS_CASES)\n",
    "total_cases = len(ANALYTICS_CASES)\n",
    "\n",
    "print(\"=== ANALYTICS EVALUATION (Layer A) ===\\n\")\n",
    "\n",
    "intent_correct = analytics_stats.get(\"intent_correct\", 0)\n",
    "intent_wrong = analytics_stats.get(\"intent_wrong\", 0)\n",
    "\n",
    "intent_accuracy = (\n",
    "    intent_correct / (intent_correct + intent_wrong)\n",
    "    if (intent_correct + intent_wrong) > 0\n",
    "    else 0.0\n",
    ")\n",
    "\n",
    "unsupported_correct = analytics_stats.get(\"unsupported_correct\", 0)\n",
    "unsupported_wrong = analytics_stats.get(\"unsupported_wrong\", 0)\n",
    "\n",
    "unsupported_accuracy = (\n",
    "    unsupported_correct / (unsupported_correct + unsupported_wrong)\n",
    "    if (unsupported_correct + unsupported_wrong) > 0\n",
    "    else 0.0\n",
    ")\n",
    "\n",
    "print(f\"Intent accuracy (supported): {intent_accuracy:.2%}\")\n",
    "print(f\"Unsupported intent correctness: {unsupported_accuracy:.2%}\")\n",
    "\n",
    "print(\"\\nRaw stats:\")\n",
    "print(dict(analytics_stats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be815036-d9b1-4552-8ec8-9877a8cc3bfc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
